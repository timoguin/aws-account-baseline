{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Definition \u00b6 aws-baseline noun English Baseline essentials for an AWS account: documented and codified. Used in a sentence: I am trying to implement some new AWS accounts using the aws-baseline spec .","title":"Definition"},{"location":"#definition","text":"aws-baseline noun English Baseline essentials for an AWS account: documented and codified. Used in a sentence: I am trying to implement some new AWS accounts using the aws-baseline spec .","title":"Definition"},{"location":"code/","text":"Code \u00b6 Instructure as code. Scripts \u00b6 Sometimes you just need a script to do something. Terraform \u00b6 TODO Examples \u00b6 TODO","title":"Code"},{"location":"code/#code","text":"Instructure as code.","title":"Code"},{"location":"code/#scripts","text":"Sometimes you just need a script to do something.","title":"Scripts"},{"location":"code/#terraform","text":"TODO","title":"Terraform"},{"location":"code/#examples","text":"TODO","title":"Examples"},{"location":"goals/","text":"Goals \u00b6 There are many steps required to properly setup a secure AWS environment. Some of theses processes require manual work (sometimes from the root user only), some can be automated with infrastructure as code, and some are a mixture of the too. As well as baseline requirements, there is also a need to define longer term goals and incremental improvements to the various components. Taken together, this can all be difficult to comprehend and to properly implement. Documentation \u00b6 The first goal of this document is to provide a detailed overview of all these processes. Processes cannot be automated with first being documented in detail. If there is not a source of truth that engineers can quickly reference, they will burn more money than necessary. Teams will over-communicate with with each other. They will make mistakes that can lead to a tangled, inconsistent mess of infrastructure. This inconsistency leads to gaps in documentation, and individual engineers will quickly become subject matter experts on various aspects within. Bob will be asked about any IAM issues, Chris will be asked about how alarms are configured, and every developer will ask how to access logs and make changes in their accounts. Reliability \u00b6 It's easier to sleep at night when there is confidence in the consistency of your infrastructure. It is essential to have a source of truth. Have you ever had questions like the following: Who changed the configuration for the load balancer? How do I use IAM to secure access to resources? How can I audit my instructure? What are the right ways to setup CloudTrail? How do I give developers freedom to experiment within their accounts with compromising essential security requirements? Why should I use tags, and what tags should I use? Many questions like this shouldn't take more work that reading sections of this document. No one should have to think much about all this. Security \u00b6 Automating all the various aspect of security with an AWS account can be a daunting task. AWS offers a large handful of services that produce logs, metrics, streams, and many integrations with other useful services. The format and delivery of these logs vary between service: some only log to S3, some can write to CloudWatch Logs, some offer notifications for log delivery, and so on. In addition to service logs from AWS, there also have to be logging pipelines for applications. These features need to be easy and convenient to use, without much thought from engineers.","title":"Goals"},{"location":"goals/#goals","text":"There are many steps required to properly setup a secure AWS environment. Some of theses processes require manual work (sometimes from the root user only), some can be automated with infrastructure as code, and some are a mixture of the too. As well as baseline requirements, there is also a need to define longer term goals and incremental improvements to the various components. Taken together, this can all be difficult to comprehend and to properly implement.","title":"Goals"},{"location":"goals/#documentation","text":"The first goal of this document is to provide a detailed overview of all these processes. Processes cannot be automated with first being documented in detail. If there is not a source of truth that engineers can quickly reference, they will burn more money than necessary. Teams will over-communicate with with each other. They will make mistakes that can lead to a tangled, inconsistent mess of infrastructure. This inconsistency leads to gaps in documentation, and individual engineers will quickly become subject matter experts on various aspects within. Bob will be asked about any IAM issues, Chris will be asked about how alarms are configured, and every developer will ask how to access logs and make changes in their accounts.","title":"Documentation"},{"location":"goals/#reliability","text":"It's easier to sleep at night when there is confidence in the consistency of your infrastructure. It is essential to have a source of truth. Have you ever had questions like the following: Who changed the configuration for the load balancer? How do I use IAM to secure access to resources? How can I audit my instructure? What are the right ways to setup CloudTrail? How do I give developers freedom to experiment within their accounts with compromising essential security requirements? Why should I use tags, and what tags should I use? Many questions like this shouldn't take more work that reading sections of this document. No one should have to think much about all this.","title":"Reliability"},{"location":"goals/#security","text":"Automating all the various aspect of security with an AWS account can be a daunting task. AWS offers a large handful of services that produce logs, metrics, streams, and many integrations with other useful services. The format and delivery of these logs vary between service: some only log to S3, some can write to CloudWatch Logs, some offer notifications for log delivery, and so on. In addition to service logs from AWS, there also have to be logging pipelines for applications. These features need to be easy and convenient to use, without much thought from engineers.","title":"Security"},{"location":"overview/","text":"Overview \u00b6 This set of documents outline the baseline essentials that should be applied in every AWS account. It is intended to be a combination of best practices documentation with a step-by-step guide on setting up an AWS account in A Right Way\u2122 (or at least that's what I'm calling it). To assist in implementing these practices, they have been codified as much as possible with Terraform. We hope to add more provisioning options in the future, as this project continues to move towards providing a set of structured data that can be consumed by various tooling, for the most truest sense of freedom.","title":"Overview"},{"location":"overview/#overview","text":"This set of documents outline the baseline essentials that should be applied in every AWS account. It is intended to be a combination of best practices documentation with a step-by-step guide on setting up an AWS account in A Right Way\u2122 (or at least that's what I'm calling it). To assist in implementing these practices, they have been codified as much as possible with Terraform. We hope to add more provisioning options in the future, as this project continues to move towards providing a set of structured data that can be consumed by various tooling, for the most truest sense of freedom.","title":"Overview"},{"location":"code/","text":"Infrastructure Code \u00b6 Document infrastructure code.","title":"Code"},{"location":"code/#infrastructure-code","text":"Document infrastructure code.","title":"Infrastructure Code"},{"location":"spec/","text":"Specification \u00b6 This document contains a high-level overview of the essentials. It contains a breakdown of different categories of requirements. Processes \u00b6 Manual \u00b6 Some actions require logging in as a root user to perform. These are typically related to initial IAM user creation, enabling/disabling certain regions, billing, and other things. Prequisites: understand the scope of any accounts being managed use a consistent naming scheme across accounts use a consistent process to manage any root account emails and passwords The following process much be performed manually, typically as the root user: root user password root user mfa disable unused regions opt in to the longer ARN format for ECS (not necessary for new accounts) Automated \u00b6 Other processes can be automated via the relevant AWS APIs. TODO: This is really just a big list, you know. . . account alias account password policy delete default vpcs service log buckets s3 access log buckets cloudtrail config iam access analyzer sns topics sqs queues iam users iam roles iam policies iam saml providers cloudwatch log groups lambda functions kinesis streams firehose delivery streams cloudwatch events processors eventbridge processors dynamodb table(s) for Terraform state locks s3 buckets for Terraform state s3 bucket replication (cross-account / cross-region) dynamodb table replication (use Global Tables) kms keys budgets s3 inventory s3 analytics service quotas Events to Capture\u00b6 root logins config changes access denied events kms key rotations and usage Common Tooling \u00b6 Every account needs tooling to move events around. logging pipelines notification pipelines Compliance \u00b6 Every account should have tooling to perform analysis for compliance audits. The necessary data should be compiled by indexing the various event sources available. Where practical, data should be streamed and processed in real time, reacting to data via automated pipelines. Data should also be indexed for historical analysis as well. In short, accounts should audit themselves, while still providing mechanisms to make the data available outside of the account. config rules cloudtrail analysis required tags Advanced \u00b6 TODO Costs \u00b6 TODO","title":"Index"},{"location":"spec/#specification","text":"This document contains a high-level overview of the essentials. It contains a breakdown of different categories of requirements.","title":"Specification"},{"location":"spec/#processes","text":"","title":"Processes"},{"location":"spec/#manual","text":"Some actions require logging in as a root user to perform. These are typically related to initial IAM user creation, enabling/disabling certain regions, billing, and other things. Prequisites: understand the scope of any accounts being managed use a consistent naming scheme across accounts use a consistent process to manage any root account emails and passwords The following process much be performed manually, typically as the root user: root user password root user mfa disable unused regions opt in to the longer ARN format for ECS (not necessary for new accounts)","title":"Manual"},{"location":"spec/#automated","text":"Other processes can be automated via the relevant AWS APIs. TODO: This is really just a big list, you know. . . account alias account password policy delete default vpcs service log buckets s3 access log buckets cloudtrail config iam access analyzer sns topics sqs queues iam users iam roles iam policies iam saml providers cloudwatch log groups lambda functions kinesis streams firehose delivery streams cloudwatch events processors eventbridge processors dynamodb table(s) for Terraform state locks s3 buckets for Terraform state s3 bucket replication (cross-account / cross-region) dynamodb table replication (use Global Tables) kms keys budgets s3 inventory s3 analytics service quotas Events to Capture\u00b6 root logins config changes access denied events kms key rotations and usage","title":"Automated"},{"location":"spec/#common-tooling","text":"Every account needs tooling to move events around. logging pipelines notification pipelines","title":"Common Tooling"},{"location":"spec/#compliance","text":"Every account should have tooling to perform analysis for compliance audits. The necessary data should be compiled by indexing the various event sources available. Where practical, data should be streamed and processed in real time, reacting to data via automated pipelines. Data should also be indexed for historical analysis as well. In short, accounts should audit themselves, while still providing mechanisms to make the data available outside of the account. config rules cloudtrail analysis required tags","title":"Compliance"},{"location":"spec/#advanced","text":"TODO","title":"Advanced"},{"location":"spec/#costs","text":"TODO","title":"Costs"},{"location":"spec/logging/","text":"Logging \u00b6 All accounts should have multiple logging pipelines available that can be used with little thought. Accounts should all have the tooling necessary to retain and analyze logs for a set retention period. All logging buckets should be replicated to another account for longer-term retention and analysis. See the s3 spec for details on bucket configurations necessary to support log collection from various sources. AWS Service Logs \u00b6 All logs should be delivered to S3 Use SNS to receive log delivery events Send events to SQS queues Subscribe Lambdas to the queues (invoked in parallel batches by SQS) Processing \u00b6 Methods of log processing and delivery will vary depending on the volume of data there is to process. Things like CloudTrail and ELB logs can become quite large with active accounts and services. Lambdas should process the messages, build a list of files to process, route the list to other destinations (if necessary), and deliver the processed log data to another S3 bucket Use Athena's \"Create Table As\" query to convert logs to parquet and manage the associated Glue catalogs. Replicate processed logs between regions Replicate processed logs to another account Controlling Cost \u00b6 Monitoring \u00b6 Use S3 Inventory reports to periodically compare the processed S3 Events with the actual objects in the bucket. This ensures that we still process any objects in the event that S3 fails to properly fire and/or delivery an event.","title":"Logging"},{"location":"spec/logging/#logging","text":"All accounts should have multiple logging pipelines available that can be used with little thought. Accounts should all have the tooling necessary to retain and analyze logs for a set retention period. All logging buckets should be replicated to another account for longer-term retention and analysis. See the s3 spec for details on bucket configurations necessary to support log collection from various sources.","title":"Logging"},{"location":"spec/logging/#aws-service-logs","text":"All logs should be delivered to S3 Use SNS to receive log delivery events Send events to SQS queues Subscribe Lambdas to the queues (invoked in parallel batches by SQS)","title":"AWS Service Logs"},{"location":"spec/logging/#processing","text":"Methods of log processing and delivery will vary depending on the volume of data there is to process. Things like CloudTrail and ELB logs can become quite large with active accounts and services. Lambdas should process the messages, build a list of files to process, route the list to other destinations (if necessary), and deliver the processed log data to another S3 bucket Use Athena's \"Create Table As\" query to convert logs to parquet and manage the associated Glue catalogs. Replicate processed logs between regions Replicate processed logs to another account","title":"Processing"},{"location":"spec/logging/#controlling-cost","text":"","title":"Controlling Cost"},{"location":"spec/logging/#monitoring","text":"Use S3 Inventory reports to periodically compare the processed S3 Events with the actual objects in the bucket. This ensures that we still process any objects in the event that S3 fails to properly fire and/or delivery an event.","title":"Monitoring"},{"location":"spec/cloudtrail/","text":"CloudTrail \u00b6 CloudTrail is an AWS service that records API calls and other events that occur within an account. Accounts should have a mechanism in place to quickly and easy analyze the large variety of API calls and events that flow through the system. Recommended \u00b6 Create a single trail for global events (IAM) in us-east-1 Create a trail in each region for read events Create a trail in each region for write events Create a trail in each region for S3 Object events Create a trail in each region for Lambda events Service Events \u00b6 https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-aws-service-specific-topics.html AWS Organizations SCPs \u00b6 If your account is part of an AWS Organization, Service Control Policies can be used to ensure use of CloudTrail, as well as ensure critical resources cannot be changed. TODO: Add example SCP for CloudTrail","title":"CloudTrail"},{"location":"spec/cloudtrail/#cloudtrail","text":"CloudTrail is an AWS service that records API calls and other events that occur within an account. Accounts should have a mechanism in place to quickly and easy analyze the large variety of API calls and events that flow through the system.","title":"CloudTrail"},{"location":"spec/cloudtrail/#recommended","text":"Create a single trail for global events (IAM) in us-east-1 Create a trail in each region for read events Create a trail in each region for write events Create a trail in each region for S3 Object events Create a trail in each region for Lambda events","title":"Recommended"},{"location":"spec/cloudtrail/#service-events","text":"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-aws-service-specific-topics.html","title":"Service Events"},{"location":"spec/cloudtrail/#aws-organizations-scps","text":"If your account is part of an AWS Organization, Service Control Policies can be used to ensure use of CloudTrail, as well as ensure critical resources cannot be changed. TODO: Add example SCP for CloudTrail","title":"AWS Organizations SCPs"},{"location":"spec/config/","text":"Config \u00b6 Config is an AWS service that tracks the configuration of resources. Any modifications to resources supported by Config will be captured and presented on a timeline, along with a diff of config changes and details about what principal modified the resource. Config Rules allow tracking compliance requirements. There is a list of built-in rules that can be enabled, and custom rules can be created via Lambda functions. Config delivers logs to S3 and sends a notification to SNS upon delivery. Rules can be configured to send notifications, perform remediation, or perform other customizable actions. Recommended \u00b6 - Built-in Rules \u00b6 TODO: Build list of recommended Config Rules Custom Rules \u00b6 TODO: Add recommendations for how to use custom rules, including any that should be enabled by default. AWS Organizations SCPs \u00b6 If your account is part of an AWS Organization, Service Control Policies can be used to ensure the usage of Config and Config Rules, as well as ensure critical resources cannot be changed. TODO: Add example SCP for Config","title":"Config"},{"location":"spec/config/#config","text":"Config is an AWS service that tracks the configuration of resources. Any modifications to resources supported by Config will be captured and presented on a timeline, along with a diff of config changes and details about what principal modified the resource. Config Rules allow tracking compliance requirements. There is a list of built-in rules that can be enabled, and custom rules can be created via Lambda functions. Config delivers logs to S3 and sends a notification to SNS upon delivery. Rules can be configured to send notifications, perform remediation, or perform other customizable actions.","title":"Config"},{"location":"spec/config/#recommended","text":"-","title":"Recommended"},{"location":"spec/config/#built-in-rules","text":"TODO: Build list of recommended Config Rules","title":"Built-in Rules"},{"location":"spec/config/#custom-rules","text":"TODO: Add recommendations for how to use custom rules, including any that should be enabled by default.","title":"Custom Rules"},{"location":"spec/config/#aws-organizations-scps","text":"If your account is part of an AWS Organization, Service Control Policies can be used to ensure the usage of Config and Config Rules, as well as ensure critical resources cannot be changed. TODO: Add example SCP for Config","title":"AWS Organizations SCPs"},{"location":"spec/iam/access-analyzer/","text":"","title":"Access analyzer"},{"location":"spec/iam/permission-boundaries/","text":"","title":"Permission boundaries"},{"location":"spec/iam/policies/","text":"","title":"Policies"},{"location":"spec/iam/roles/","text":"","title":"Roles"},{"location":"spec/infrastructure-as-code/terraform-remote-state/","text":"","title":"Terraform remote state"},{"location":"spec/route53/","text":"Route53 \u00b6 Enable query logging to CloudWatch Logs Steam logs to Kinesis Firehose for further processing","title":"Route53"},{"location":"spec/route53/#route53","text":"Enable query logging to CloudWatch Logs Steam logs to Kinesis Firehose for further processing","title":"Route53"},{"location":"spec/s3/","text":"S3 \u00b6 Buckets for AWS service logs in each region (w/ S3 Object Lock) Bucket for AWS S3 access logs in each region (w/ S3 Object Lock) Replicate logging buckets to another account for longer retention and auditing S3 access logs are an outlier in that the object ownership and ACLs are special for the S3 service. Cross-account access requires role assumption and changes to object ownership. Patterns \u00b6 Single Bucket \u00b6 Bucket name: - -aws-service-logs- Bucket name example: 123456789012-dev-aws-service-logs-us-east-1 Log types and prefixes: CloudTrail: cloudtrail/ Config: config/ ALB/ELB/NLB: lb/, alb/, elb/, nlb/ (pull from any and process the same) Multiple Per-Service Buckets \u00b6 CloudTrail: - -cloudtrail-logs- Config: - -config-logs- ELB: - -elb-access-logs- S3: - -s3-access-logs- Delivery Notications \u00b6 CloudTrail: send to SNS Config: send to SNS ELB: S3 events to SNS S3: S3 events to SNS Plumbing \u00b6 Use S3 Events (optional) -> SNS -> SQS to retain a queue of events to process SNS can also fan out to other destinations SNS Event Forking to SAM applications (parallel processing and filtering) SNS topics and SQS queues send failures to deadletter queues Lambda to process the deadletter queues Security \u00b6 Each bucket only allows access from specific AWS services in the same region All buckets encrypted with KMS, forced via the bucket policy Public access blocked Object Lock enabled (use compliance mode) VPC Endpoint for S3 Special Notes for S3 Access Logs \u00b6 Logging buckets for S3 access logs cannot be encrypted. The bucket owner for the objects is also specific to the logging service. Objects cannot be accessed from another account without assuming a role into the account with the bucket. To allow these logs to be processed in the same manner as other service logs, use a Lambda (or some other form of automation) to copy the logs to another bucket and update the object ownerships. NOTE: Research S3 bucket replication in the context of S3 access logs. Could enable getting around the need for Lambda processing. Bucket Policies \u00b6 Specific AWS services vary on how to grant permissions to use the buckets: CloudTrail uses a service principal (cloudtrail.amazonaws.com) to put logs and check bucket ACLs Config uses an IAM role to do the same ELB uses AWS account IDs (root) for the ELB service, different ones in each region Durability \u00b6 Replication (replicate buckets to another region and/or account for longer-term storage and analysis) Versioning enabled (required for replication and object lock) Cost Controls \u00b6 - Lifecycle policies to transition objects into cheaper storage (and/or eventually delete) \u00b6","title":"S3"},{"location":"spec/s3/#s3","text":"Buckets for AWS service logs in each region (w/ S3 Object Lock) Bucket for AWS S3 access logs in each region (w/ S3 Object Lock) Replicate logging buckets to another account for longer retention and auditing S3 access logs are an outlier in that the object ownership and ACLs are special for the S3 service. Cross-account access requires role assumption and changes to object ownership.","title":"S3"},{"location":"spec/s3/#patterns","text":"","title":"Patterns"},{"location":"spec/s3/#single-bucket","text":"Bucket name: - -aws-service-logs- Bucket name example: 123456789012-dev-aws-service-logs-us-east-1 Log types and prefixes: CloudTrail: cloudtrail/ Config: config/ ALB/ELB/NLB: lb/, alb/, elb/, nlb/ (pull from any and process the same)","title":"Single Bucket"},{"location":"spec/s3/#multiple-per-service-buckets","text":"CloudTrail: - -cloudtrail-logs- Config: - -config-logs- ELB: - -elb-access-logs- S3: - -s3-access-logs-","title":"Multiple Per-Service Buckets"},{"location":"spec/s3/#delivery-notications","text":"CloudTrail: send to SNS Config: send to SNS ELB: S3 events to SNS S3: S3 events to SNS","title":"Delivery Notications"},{"location":"spec/s3/#plumbing","text":"Use S3 Events (optional) -> SNS -> SQS to retain a queue of events to process SNS can also fan out to other destinations SNS Event Forking to SAM applications (parallel processing and filtering) SNS topics and SQS queues send failures to deadletter queues Lambda to process the deadletter queues","title":"Plumbing"},{"location":"spec/s3/#security","text":"Each bucket only allows access from specific AWS services in the same region All buckets encrypted with KMS, forced via the bucket policy Public access blocked Object Lock enabled (use compliance mode) VPC Endpoint for S3","title":"Security"},{"location":"spec/s3/#special-notes-for-s3-access-logs","text":"Logging buckets for S3 access logs cannot be encrypted. The bucket owner for the objects is also specific to the logging service. Objects cannot be accessed from another account without assuming a role into the account with the bucket. To allow these logs to be processed in the same manner as other service logs, use a Lambda (or some other form of automation) to copy the logs to another bucket and update the object ownerships. NOTE: Research S3 bucket replication in the context of S3 access logs. Could enable getting around the need for Lambda processing.","title":"Special Notes for S3 Access Logs"},{"location":"spec/s3/#bucket-policies","text":"Specific AWS services vary on how to grant permissions to use the buckets: CloudTrail uses a service principal (cloudtrail.amazonaws.com) to put logs and check bucket ACLs Config uses an IAM role to do the same ELB uses AWS account IDs (root) for the ELB service, different ones in each region","title":"Bucket Policies"},{"location":"spec/s3/#durability","text":"Replication (replicate buckets to another region and/or account for longer-term storage and analysis) Versioning enabled (required for replication and object lock)","title":"Durability"},{"location":"spec/s3/#cost-controls","text":"","title":"Cost Controls"},{"location":"spec/s3/#-lifecycle-policies-to-transition-objects-into-cheaper-storage-andor-eventually-delete","text":"","title":"- Lifecycle policies to transition objects into cheaper storage (and/or eventually delete)"},{"location":"spec/tagging/","text":"Tagging Standards \u00b6","title":"Tagging Standards"},{"location":"spec/tagging/#tagging-standards","text":"","title":"Tagging Standards"}]}